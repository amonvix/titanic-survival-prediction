Titanic Survival Prediction ğŸ§ âš™ï¸














ğŸš€ Overview

Titanic Survival Prediction is a machine learningâ€“powered API that predicts whether a passenger would have survived the Titanic disaster based on demographic and ticket data.
Built with FastAPI and trained on the Kaggle Titanic dataset, this project serves as a foundation for a full MLOps workflow â€” including model training, API serving, Docker containerization, CI/CD automation, and future deployment via Fly.io and AWS ECS using Terraform.

ğŸ§© Current Status
Component	Status	Technology
Data Processing	âœ… Complete	pandas, NumPy
Model Training	âœ… Functional	scikit-learn, TensorFlow
API	âœ… Functional	FastAPI
Docker	âœ… Ready	Dockerfile
CI/CD	ğŸš§ In Progress	GitHub Actions
Infrastructure as Code	ğŸš§ In Progress	Terraform (AWS ECS planned)
Deployment	ğŸš§ Upcoming	Fly.io / AWS
Frontend	âŒ Not yet implemented	
ğŸ§  Architecture
Current Pipeline
flowchart LR
    D[Raw Titanic Data] --> P[Preprocessing & Feature Encoding]
    P --> M[Model Training - Scikit-learn / TensorFlow]
    M --> F[Model Saved as pipeline.pkl]
    F --> A[FastAPI /predict Endpoint]
    A --> R[JSON Response with Probability + Prediction]

Planned Full MLOps Flow
flowchart LR
    subgraph Dev["Local Dev"]
        Code[FastAPI + ML Training]
        Tests[Pytest / Ruff / Mypy]
    end

    Code -->|git push| Repo[(GitHub Repo)]

    subgraph CI["GitHub Actions"]
        CI1[Lint & Type Check]
        CI2[Run Tests]
        CI3[Build Docker Image]
        CI4[Push Image to GHCR/ECR]
        CI5[Deploy to Fly.io or AWS ECS]
    end

    Repo --> CI1 --> CI2 --> CI3 --> CI4 --> CI5

    subgraph Cloud["Cloud Runtime"]
        API[FastAPI App Container]
        Model[Loaded Model.pkl]
        Logs[Structured Logs]
        Metrics[Monitoring (Prometheus/Grafana - Planned)]
    end

    CI5 --> API --> Model --> Logs

ğŸ“‚ Project Structure
titanic-survival-prediction/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â””â”€â”€ routers/main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ titanic.csv
â”‚   â””â”€â”€ titanic_clean.csv
â”‚
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â””â”€â”€ provider.tf
â”‚
â”œâ”€â”€ models/pipeline.pkl
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_pipeline.py
â”‚   â”œâ”€â”€ save_sklearn_model.py
â”‚   â””â”€â”€ train_model.py
â”‚
â”œâ”€â”€ aws-oidc-setup/
â”‚   â”œâ”€â”€ policy-ecr.json
â”‚   â””â”€â”€ trust-policy.json
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ mypy.ini
â”œâ”€â”€ pytest.ini
â””â”€â”€ .github/workflows/ci.yml

ğŸ§ª Running Locally

Clone the repository and set up your environment:

git clone https://github.com/Amonvix/titanic-survival-prediction.git
cd titanic-survival-prediction
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt


Run the API locally:

uvicorn app.routers.main:app --reload


Then open your browser at http://localhost:8000/docs
 to access Swagger UI.

ğŸ§¾ Example Request
curl -X POST http://localhost:8000/predict/ \
  -H "Content-Type: application/json" \
  -d '{
        "age": 28,
        "sex": "male",
        "pclass": 3,
        "sibsp": 0,
        "parch": 0,
        "fare": 7.25,
        "embarked": "Southampton",
        "deck": "Unknown"
      }'


Response:

{
  "survived_probability": 0.237,
  "survived": false
}

ğŸ§± Docker

Build and run the container:

docker build -t titanic-api .
docker run -d -p 8000:8000 titanic-api


Then test via:

curl http://localhost:8000/predict/

ğŸ§® Model Training

The model is trained on the Kaggle Titanic dataset with preprocessing, encoding, and normalization handled by a Scikit-learn pipeline.
Trained models are serialized to models/pipeline.pkl.

Key scripts:

train_model.py â€” trains the model

create_pipeline.py â€” builds preprocessing & inference pipeline

save_sklearn_model.py â€” saves the model for API use

Future integration with TensorFlow will allow hybrid or ensemble training.

ğŸ§° CI/CD & Infrastructure

CI/CD: GitHub Actions (.github/workflows/ci.yml) handles linting, testing, and build automation.

Docker Build: Prepares production-ready container image for deployment.

Terraform: Declarative IaC setup under infra/terraform for AWS ECS / ECR resources.

OIDC Setup: aws-oidc-setup/ enables secure GitHub â†’ AWS role assumption for deployments.

Planned next steps:

âœ… Add automated tests and coverage reports

âœ… Push Docker image to GHCR / ECR

ğŸš€ Deploy on Fly.io / AWS ECS via Terraform

ğŸ§­ Roadmap

 Model training and serialization

 API for prediction

 Dockerized application

 CI/CD pipeline via GitHub Actions

 Infrastructure provisioning via Terraform

 Deploy to Fly.io / AWS ECS

 Add Prometheus/Grafana metrics

 Unit & integration tests

ğŸ§‘â€ğŸ’» Author

Daniel Pedroso (Amonvix)
GitHub
 â€¢ LinkedIn

ğŸ“œ License

Licensed under the MIT License.
Built with passion and precision ğŸ§©